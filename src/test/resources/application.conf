//myValue = ${?myvalue}
setl.environment = ${app.environment}

test {

  //  myValue2 = ${myValue}"-loaded"

  excel {
    storage = "EXCEL"
    path = "src/test/resources/test_config_excel.xlsx"
    useHeader = "true"
    treatEmptyValuesAsNulls = "true"
    inferSchema = "false"
    schema = "partition1 INT, partition2 STRING, clustering1 STRING, value LONG"
    saveMode = "Overwrite"
  }

  cassandra {
    storage = "CASSANDRA"
    keyspace = "test_space"
    table = "test_spark_connector2"
    partitionKeyColumns = ["partition1", "partition2"]
    clusteringKeyColumns = ["clustering1"]
  }

  cassandraWithoutClustering {
    storage = "CASSANDRA"
    keyspace = "test_space"
    table = "test_spark_connector2"
    partitionKeyColumns = ["partition1", "partition2"]
  }

  parquet {
    storage = "PARQUET"
    path = "src/test/resources/test_config_parquet"  // must be absolute path
    table = "test_config"
    saveMode = "Append"
  }

  csv {
    storage = "CSV"
    path = "src/test/resources/test_config_csv"
    inferSchema = "true"
    delimiter = ";"
    header = "true"
    saveMode = "Append"
  }

  json {
    storage = "JSON"
    path = "src/test/resources/test_config_json"
    saveMode = "Append"
  }
}

connector {
  excel {
    storage = "EXCEL"
    path = "src/test/resources/test_config_excel2.xlsx"
    useHeader = "true"
    treatEmptyValuesAsNulls = "true"
    inferSchema = "false"
    schema = "partition1 INT, partition2 STRING, clustering1 STRING, value LONG"
    saveMode = "Overwrite"
  }

  cassandra {
    storage = "CASSANDRA"
    keyspace = "test_space"
    table = "test_spark_connector3"
    partitionKeyColumns = ["partition1", "partition2"]
    clusteringKeyColumns = ["clustering1"]
  }

  parquet {
    storage = "PARQUET"
    path = "src/test/resources/test_config_parquet2"  // must be absolute path
    table = "test_config2"
    saveMode = "Append"
  }

  csv {
    storage = "CSV"
    path = "src/test/resources/test_config_csv2"
    inferSchema = "true"
    delimiter = ";"
    header = "true"
    saveMode = "Append"
  }

  csvWithSchema {
    storage = "CSV"
    path = "src/test/resources/test_csv_connector_with_schema"
    schema = "partition2 STRING, clustering1 STRING, partition1 INT, value LONG"
    inferSchema = "true"
    delimiter = ";"
    header = "true"
    saveMode = "Append"
  }

  csvWithSchema2 {
    storage = "CSV"
    path = "src/test/resources/test_csv_connector_with_schema2"
    schema = "partition2 STRING, value LONG, clustering1 STRING, partition1 INT"
    inferSchema = "true"
    delimiter = ";"
    header = "true"
    saveMode = "Append"
  }

  dynamo {
    storage = "DYNAMODB"
    region = ""
    table = ""
    saveMode = "Overwrite"
    spark.hadoop.fs.s3a.aws.credentials.provider = "com.amazonaws.auth.DefaultAWSCredentialsProviderChain"
  }
}

connectorBuilder {
  excel {
    storage = "EXCEL"
    path = "src/test/resources/test_config_excel3.xlsx"
    useHeader = "true"
    treatEmptyValuesAsNulls = "true"
    inferSchema = "false"
    schema = "partition1 INT, partition2 STRING, clustering1 STRING, value LONG"
    saveMode = "Overwrite"
  }

  cassandra {
    storage = "CASSANDRA"
    keyspace = "test_space"
    table = "test_spark_connector4"
    partitionKeyColumns = ["partition1", "partition2"]
    clusteringKeyColumns = ["clustering1"]
  }

  parquet {
    storage = "PARQUET"
    path = "src/test/resources/test_config_parquet3"  // must be absolute path
    table = "test_config3"
    saveMode = "Append"
  }

  csv {
    storage = "CSV"
    path = "src/test/resources/test_config_csv3"
    inferSchema = "true"
    delimiter = ";"
    header = "true"
    saveMode = "Append"
  }

  json {
    storage = "JSON"
    path = "src/test/resources/test_config_json3"
    saveMode = "Append"
  }

  wrong_csv {
    storage = "BLABLA"
    path = "src/test/resources/test_config_csv3"
    inferSchema = "true"
    delimiter = ";"
    header = "true"
    saveMode = "Append"
  }

  wrong_csv2 {
    storage = "OTHER"
    path = "src/test/resources/test_config_csv3"
    inferSchema = "true"
    delimiter = ";"
    header = "true"
    saveMode = "Append"
  }
}

repoBuilder {
  excel {
    storage = "EXCEL"
    path = "src/test/resources/test_config_excel4.xlsx"
    useHeader = "true"
    treatEmptyValuesAsNulls = "true"
    inferSchema = "false"
    schema = "partition1 INT, partition2 STRING, clustering1 STRING, value LONG"
    saveMode = "Overwrite"
  }

  cassandra {
    storage = "CASSANDRA"
    keyspace = "test_space"
    table = "test_spark_connector5"
    partitionKeyColumns = ["partition1", "partition2"]
    clusteringKeyColumns = ["clustering1"]
  }

  parquet {
    storage = "PARQUET"
    path = "src/test/resources/test_config_parquet4"  // must be absolute path
    table = "test_config4"
    saveMode = "Append"
  }

  csv {
    storage = "CSV"
    path = "src/test/resources/test_config_csv4"
    inferSchema = "true"
    delimiter = ";"
    header = "true"
    saveMode = "Append"
  }
}

psql {
  test {
    storage = "JDBC"
    url = "jdbc:postgresql://localhost:5432/framework_dev"
    dbtable = "unittest"
    saveMode = "Overwrite"
    user = "postgres"
    password = "postgres"
  }
}

schemaConverter {
  testFile {
    storage = "CSV"
    path = "src/test/resources/test_schema_converter.csv"
    inferSchema = "true"
    header = "true"
    saveMode = "ErrorIfExists"
  }
}